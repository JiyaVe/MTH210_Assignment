theta <- seq(-1,6,length = 5e2)
?seq(-1,6,length = 5e2)
theta <- seq(-1,6,length = 5e2)
plot(theta, like(theta), type = 'l',
xlab = expression(theta), ylab = "Likelihood")
#Likelihood function
x1 <- 2
x2 <- 3
like <- function(theta)
{
dnorm(x1,mean = theta)*dnorm(x2,mean = theta)
}
theta <- seq(-1,6,length = 5e2)
plot(theta, like(theta), type = 'l',
xlab = expression(theta), ylab = "Likelihood")
abline(v = (x1 + x2)/2, col = "blue", lty = 2)
alpha <- 10
dat <- rgamma(100, shape = alpha, rate = 2)
alpha.grid <- seq(6, 15, length = 5e2)
alpha.grid
like <- function(alpha)
{
prod(dgamma(dat, shape = alpha, rate = 2))
}
loglike <- function(alpha)
{
sum(dgamma(dat, shape = alpha, rate = 2, log = TRUE))
}
cal.like <- numeric(length = length(alpha.grid))
cal.loglike <- numeric(length = length(alpha.grid))
## calculate log likelihood for each value of alpha
for(i in 1:length(alpha.grid))
{
cal.like[i] <- like(alpha.grid[i])
cal.loglike[i] <- loglike(alpha.grid[i])
}
par(mfrow = c(1,2))
plot(alpha.grid, cal.like, type = 'l',
xlab = expression(alpha), ylab = "Likelihood")
2
plot(alpha.grid, cal.loglike, type = 'l',
xlab = expression(alpha), ylab = "Log Likelihood")
par(mfrow = c(1,2))
plot(alpha.grid, cal.like, type = 'l',
xlab = expression(alpha), ylab = "Likelihood")
plot(alpha.grid, cal.loglike, type = 'l',
xlab = expression(alpha), ylab = "Log Likelihood")
############################
par(mfrow = c(1,1))
# same value
which.max(cal.like)
which.max(cal.loglike)
alpha.grid[which.max(cal.like)]
data(cars)
X <- cars$speed
X <- cbind(1, X) # add a column
y <- cars$dist
beta.mle <- solve(t(X) %*% X) %*% t(X) %*% y
beta.mle
fuel2001 <- read.csv("https://dvats.github.io/assets/fuel2001.csv", row.names = 1)
fuel2001 <- read.csv("https://dvats.github.io/assets/fuel2001.csv", row.names = 1)
head(fuel2001)
X <- as.matrix(fuel2001[, -2])
X <- as.matrix(fuel2001[, -2])
X <- cbind(1, X)
y <- fuel2001$FuelC
n <- length(y)
XtX <- t(X) %*% X
?qr.solve(XtX, tol = 1e-20)
XtX.inv <- qr.solve(XtX, tol = 1e-20)
beta.mle <- XtX.inv %*% t(X) %*% y
beta.mle
qr.solve(XtX, tol = 1e-20)
X
sig2 <- t(y - X%*%beta.mle) %*% (y - X%*%beta.mle)/n
sig2
lam <- 1
beta.ridge <- solve(t(X) %*% X + diag(lam, dim(X)[2])) %*% t(X) %*% y
beta.ridge
set.seed(1)
n <- 50
p <- 5
sigma2.star <- 1/2
beta.star <- rnorm(p)
X <- cbind(1, matrix(rnorm(n*(p-1)), nrow = n, ncol = (p-1)))
matrix(rnorm(n*(p-1)
)
/
.
p <- 5
rnorm(p)
?rnorm(p)
X <- cbind(1, matrix(rnorm(n*(p-1)), nrow = n, ncol = (p-1)))
y <- X %*% beta.star + rnorm(n, mean = 0, sd = sqrt(sigma2.star))
# MLE
beta.mle <- solve(t(X) %*% X) %*% t(X) %*% y
cbind(beta.mle, beta.star)
n <- 500
X <- cbind(1, matrix(rnorm(n*(p-1)), nrow = n, ncol = (p-1)))
y <- X %*% beta.star + rnorm(n, mean = 0, sd = sqrt(sigma2.star))
beta.mle <- solve(t(X) %*% X) %*% t(X) %*% y
cbind(beta.mle, beta.star)
set.seed(1)
n <- 50
p <- 100
sigma2.star <- 1/2
beta.star <- rnorm(p)
X <- cbind(1, matrix(rnorm(n*(p-1)), nrow = n, ncol = (p-1)))
y <- X %*% beta.star + rnorm(n, mean = 0, sd = sqrt(sigma2.star))
beta.01 <- solve(t(X) %*% X + diag(.01, p)) %*% t(X) %*% y
beta.1 <- solve(t(X) %*% X + diag(.1, p)) %*% t(X) %*% y
beta1 <- solve(t(X) %*% X + diag(1, p)) %*% t(X) %*% y
beta10 <- solve(t(X) %*% X + diag(10, p)) %*% t(X) %*% y
Allbetas <- cbind(beta.01,beta.1, beta1, beta10)
head(Allbetas, 10) # only looking at the first rows columns
Allbetas
##########################
set.seed(100)
alpha <- 5 #true value of alpha
n <- 10 # actual data size is small first
dat <- rgamma(n, shape = alpha, rate = 1)
library(pracma) #for psi function
?psi
# will save alpha_k sequences
alpha_newton <- numeric()
epsilon <- 1e-8 #some tolerance level preset
alpha_newton[1] <- 2 #alpha_0
library(pracma) #for psi function
# will save alpha_k sequences
alpha_newton <- numeric()
epsilon <- 1e-8 #some tolerance level preset
alpha_newton[1] <- 2 #alpha_0
count <- 1
tol <- 100 # large number
while(tol > epsilon)
{
count <- count + 1
#first derivative
f.prime <- -n*psi(k = 0, alpha_newton[count - 1]) + sum(log(dat))
#second derivative
f.dprime <- -n*psi(k = 1, alpha_newton[count - 1])
alpha_newton[count] <- alpha_newton[count - 1] - f.prime/f.dprime
tol <- abs(alpha_newton[count] - alpha_newton[count-1])
}
alpha_newton
log.like <- numeric(length = 100)
# The NR methods estimates the MLE. Here the
# blue and red lines will not match because
# the data is not large enough for the consistency of
# the MLE to kick in.
#Plot the log.likelihood for different values of alpha
alpha.grid <- seq(0, 10, length = 100)
for(i in 1:100)
{
log.like[i] <- sum(dgamma(dat, shape = alpha.grid[i], log = TRUE))
}
plot(alpha.grid, log.like, type = 'l', xlab = expression(alpha), ylab = "Log Likelihood")
plot(alpha.grid, log.like, type = 'l', xlab = expression(alpha), ylab = "Log Likelihood")
abline(v = alpha, col = "red", lty = 2)
for(t in 1:count)
{
points(alpha_newton[t], sum(dgamma(dat, shape = alpha_newton[t], log = TRUE)), pch = 16)
}
abline(v = tail(alpha_newton[count]), col = "blue", lty = 2)
legend("bottomright", legend = c("Likelihood", "Truth", "MLE"), lty = c(1,2,2), col = c("black")
legend("bottomright", legend = c("Likelihood", "Truth", "MLE"), lty = c(1,2,2), col = c("black"))
legend("bottomright", legend = c("Likelihood", "Truth", "MLE"), lty = c(1,2,2), col = c("black"))
legend("bottomright", legend = c("Likelihood", "Truth", "MLE"), lty = c(1,2,2), col = c("black","blue","red"))
legend("bottomright", legend = c("Likelihood", "Truth", "MLE"), lty = c(1,2,2), col = c("black","red","blue"))
f.grad <- function(x) -sin(x)
f.hessian <- function(x) -cos(x)
tol <- 1e-10
compare <- 100
iter <- 1
xk <- c() # will store sequence here
xk[1] <- .5 # starting value
compare <- abs(xk[iter] - xk[item - 1])
tol <- 1e-10
compare <- 100
iter <- 1
xk <- c() # will store sequence here
xk[1] <- .5 # starting value
while(compare > tol)
{
iter <- iter + 1 # tracking iterations
gradient <- f.grad(xk[iter - 1])
hessian <- f.hessian(xk[iter - 1])
xk[iter] <- xk[iter - 1] - gradient/hessian
compare <- abs(xk[iter] - xk[item - 1])
}
iter
while(compare > tol)
{
iter <- iter + 1 # tracking iterations
gradient <- f.grad(xk[iter - 1])
hessian <- f.hessian(xk[iter - 1])
xk[iter] <- xk[iter - 1] - gradient/hessian
compare <- abs(xk[iter] - xk[iter- 1])
}
iter
xk[iter]
f.grad <- function(x) -sin(x)
f.hessian <- function(x) -cos(x)
tol <- 1e-10
compare <- 100
iter <- 1
xk <- c() # will store sequence here
xk[1] <- pi/2 # starting value
while(compare > tol)
{
iter <- iter + 1 # tracking iterations
gradient <- f.grad(xk[iter - 1])
hessian <- f.hessian(xk[iter - 1])
xk[iter] <- xk[iter - 1] - gradient/hessian
compare <- abs(xk[iter] - xk[iter- 1])
}
f.grad <- function(x) -sin(x)
f.hessian <- function(x) -cos(x)
tol <- 1e-10
compare <- 100
iter <- 1
xk <- c() # will store sequence here
xk[1] <- .5 # starting value
while(compare > tol)
{
iter <- iter + 1 # tracking iterations
gradient <- f.grad(xk[iter - 1])
hessian <- f.hessian(xk[iter - 1])
xk[iter] <- xk[iter - 1] - gradient/hessian
compare <- abs(xk[iter] - xk[iter- 1])
}
iter
xk[iter]
f.grad <- function(x) -sin(x)
f.hessian <- function(x) -cos(x)
tol <- 1e-10
compare <- 100
iter <- 1
xk <- c() # will store sequence here
xk[1] <- .5 # starting value
while(compare > tol)
{
iter <- iter + 1 # tracking iterations
gradient <- f.grad(xk[iter - 1])
hessian <- f.hessian(xk[iter - 1])
xk[iter] <- xk[iter - 1] - gradient/hessian
compare <- abs(f.grad(xk[iter]))
}
iter
xk[iter]
f.grad <- function(x) -sin(x)
f.hessian <- function(x) -cos(x)
tol <- 1e-10
compare <- 100
iter <- 1
xk <- c() # will store sequence here
xk[1] <- .5 # starting value
while(compare > tol)
{
iter <- iter + 1 # tracking iterations
gradient <- f.grad(xk[iter - 1])
hessian <- f.hessian(xk[iter - 1])
xk[iter] <- xk[iter - 1] - gradient/hessian
compare <- abs(f.grad(xk[iter-1]))
}
iter
xk[iter]
foo <- seq(-pi, 3*pi, length = 1e3)
plot(foo, cos(foo), type = 'l')
# putting the iterates in light color
points(xk, cos(xk), pch = 16, col = adjustcolor("blue", alpha.f = .3))
points(xk[iter], cos(xk[iter]), pch = 16, col = "blue") # putting the final value in dark
foo <- seq(-pi, 3*pi, length = 1e3)
plot(foo, cos(foo), type = 'l')
# putting the iterates in light color
points(xk, cos(xk), pch = 16, col = adjustcolor("blue", alpha.f = .3))
foo <- seq(-pi, 3*pi, length = 1e3)
plot(foo, cos(foo), type = 'l')
# putting the iterates in light color
points(xk, cos(xk), pch = 16, col = adjustcolor("blue", alpha.f = .3))
###############################
tol <- 1e-10
compare <- 100
iter <- 1
xk <- c() # will store sequence here
gradient <- c() # will store gradient sequence here
xk[1] <- .5 # starting value
gradient[1] <- f.grad(xk[1])
t <- 1
while(compare > tol && iter < 100)
{
iter <- iter + 1 # tracking iterations
gradient[iter] <- f.grad(xk[iter - 1])
xk[iter] <- xk[iter - 1] + t*gradient[iter]
compare <- abs(gradient[iter])
}
iter
xk[iter] # GA last iterate.
plot.ts(gradient) ##plot of gradients wrt to time
plot.ts(gradient) ##plot of gradients wrt to time
plot.ts(gradient) ##plot of gradients wrt to time
###############################
tol <- 1e-10
compare <- 100
iter <- 1
xk <- c() # will store sequence here
gradient <- c() # will store gradient sequence here
xk[1] <- .5 # starting value
gradient[1] <- f.grad(xk[1])
t <- 10
while(compare > tol && iter < 100)
{
iter <- iter + 1 # tracking iterations
gradient[iter] <- f.grad(xk[iter - 1])
xk[iter] <- xk[iter - 1] + t*gradient[iter]
compare <- abs(gradient[iter])
}
iter
xk[iter] # GA last iterate.
plot.ts(gradient) ##plot of gradients wrt to time
titanic <- read.csv("https://dvats.github.io/assets/titanic.csv")
head(titanic)
rep(0,p)
?glm(y ~ X - 1, family = poisson)
?sapply
rbinom(1,2271,0.5)
rbinom(1,2271,0.5)/2271
rbinom(1,2271,0.5)/2271
rbinom(1,2271,0.5)/2271
rbinom(1,2271,0.5)/2271
rbinom(1,2271,0.5)/2271
rbinom(1,2271,0.5)/2271
rbinom(1,2271,0.5)/2271
rbinom(1,2271,0.5)/2271
results<-rbinom(1000000,2271, 0.50)/2271 # million sample proportions,
mean(results);sd(results) # each having n =2271 and pi=0.50
(results)
hist(results) # histogram of the million sample proportions
pois_CLT <- function(n, mu, B) {
# n: vector of 2 sample sizes [e.g. n <- c(10, 100)]
# mu: mean parameter of Poisson distribution
# B: number of simulated random samples from the Poisson
par(mfrow = c(2, 2))
for (i in 1:2){
Y <- numeric(length=n[i]*B)
Y <- matrix(rpois(n[i]*B, mu), ncol=n[i])
Ymean <- apply(Y, 1, mean) # or, can do this with rowMeans(Y)
barplot(table(Y[1,]), main=paste("n=", n[i]), xlab="y",
col="lightsteelblue") # sample data dist. for first sample
hist(Ymean, main=paste("n=",n[i]), xlab=expression(bar(y)),
col="lightsteelblue") # histogram of B sample mean values
} }
# implement:with 100000 random sample sizes of 10 and 100, mean = 0.7
n <- c(10, 100)
pois_CLT(n, 0.7, 100000)
sample(1:6,1)
sample(1:6,1)
sample(1:6,1)
sample(1:6,1)
sample(1:6,1)
sample(1:6,1)
sample(1:6,1)
y1 <- sample(1:6,1)
y2 <= sample(1:6,1)
while(n!=36){
c(y1,y2) <- c(sample(1:6,1),sample(1:6,1))
n <- n-1
}
n = 1
while(n!=36){
c(y1,y2) <- c(sample(1:6,1),sample(1:6,1))
n <- n+1
}
n = 1
while(n!=36){
(y1,y2) <- c(sample(1:6,1),sample(1:6,1))
n = 1
while(n!=36){
y1 <- sample(1:6,1)
y2 <- sample(1:6,1)
n <- n+1
}
y1 <-
library(binom)
install.packages(binom)
install.packages("binom")
library(binom)
Q <- matrix(c((1/6^(1/2)*(1,2,1)),(!/3^(1/2)*(1,-1,1)),(2^(1/2)*(-1/2,0,1/2))),nrow = 3,ncol = 3)
Q <- matrix(c((1/6^(1/2)*(1,2,1)),(1/3^(1/2)*(1,-1,1)),(2^(1/2)*(-1/2,0,1/2))),nrow = 3,ncol = 3)
Q <- matrix(c(c(1/6^(1/2)*(1,2,1)),c(1/3^(1/2)*(1,-1,1)),c(2^(1/2)*(-1/2,0,1/2))),nrow = 3,ncol = 3)
(1/6^(1/2)*(1,2,1)
c1 <- 1/?sqroot()
c1 <- 1/sqrt(6),2/sqrt(6),1/sqrt(6)
c1 <- c(1/sqrt(6),2/sqrt(6),1/sqrt(6))
c3 <- c(-1/sqrt(2),0,1/sqrt(2))
c1 <- c(1/sqrt(6),2/sqrt(6),1/sqrt(6))
c2 <- c(1/sqrt(3),-1/sqrt(3),1/sqrt(3))
c3 <- c(-1/sqrt(2),0,1/sqrt(2))
Q <- matrix(c(c1,c2,c3),3,3)
Q
v = eigen(Q)
norm(v[2])
v = eigen(Q)
v
norm(v(2))
v <- v$values
v <- eigen(Q)
v <- v$values
v
norm(v[2])
v[2]
v <- numeric(v$values)
v <- eigen(Q)
v <- numeric(v$values)
v <- v$values
v[2]
norm(v[2])
v <- numeric(v)
v <- numeric(3)
v
v <- v$values
v <- eigen(Q)
v <- v$values
class(v)
v
fraction(v)
x <-rpois(50,5)
mean(x)
predict.y(rnorm(100))
predict.y <- function(x)
{
x <- as.matrix(x)
load("fit_params.Rdata")
f.x <- x %*% beta.ridge
return(f.x)
}
predict.y(rnorm(100))
setwd("~/")
predict.y(rnorm(100))
setwd("C:/Users/jiyav/OneDrive - IIT Kanpur/assignment-4-JiyaVe-1")
predict.y(rnorm(100))
x <- rnorm(100)
load("fit_params.Rdata")
beta.ridge
class(beta.ridge)
class(beta.ridge)
f.x <- x %*% beta.ridge
x <- rnorm(100)
f.x <- x %*% beta.ridge
x <- rnorm(100)
predict.y(x)
class(x)
class(beta.ridge)
class(x)
predict.y <- function(x)
{
x <- as.matrix(x)
load("fit_params.Rdata")
f.x <- x %*% beta.ridge
return(f.x)
}
x <- rnorm(100)
predict.y(x)
### ONLY past your predict.y function here
# x = vector of length 100
predict.y <- function(x)
{
x <- as.matrix(x)
load("fit_params.Rdata")
f.x <- x %*% beta.ridge
return(f.x)
}
X <- read.csv("assign4_train.csv")
X <- X[,-1]
predict.y(X)
x <- as.matrix(data[,2:101])
x
predict.y(rnorm(100))
x <- rnorm(100)
predict.y(x)
load("fit_params.Rdata")
class(beta.ridge)
class(x)
f.x <- x %*% beta.ridge
# x = vector of length 100
predict.y <- function(x)
{
x <- as.matrix(x)
load("fit_params.Rdata")
beta.ridge <- as.numeric(beta.ridge)
f.x <- x %*% beta.ridge
return(f.x)
}
X <- read.csv("assign4_train.csv")
X <- X[,-1]
predict.y(X)
x <- as.matrix(data[,2:101])
x
x <- rnorm(100)
predict.y(rnorm(100))
predict.y(x)
predict.y <- function(x)
{
load("fit_params.Rdata")
x <- as.matrix(x)
f.x <- x %*% beta.ridge
return(f.x)
}
x <- rnorm(100)
predict.y(x)
class(beta.ridge)
class(x)
